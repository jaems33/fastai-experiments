{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding `fit_one_cycle(max_lr)`\n",
    "\n",
    "This notebook is exploring how fastai determines the learning rate when you pass a learning rate in a `fit_one_cycle` code.\n",
    "\n",
    "**Summary** The `slice` you provide `fit_one_cycle` gets converted into a numpy list of values. If you provide a `float` instead, it remains a `float`. \n",
    "\n",
    "The `slice` is predominantly used so that they can calculate the lower and upper bounds of the range of learning rate values. If the start value of `slice` is not provided, e.g. `slice(0, 0.03)` or `slice(None, 0.03)` then `slice.end` aka `0.03` is used in conjuction with the number of layer groups to provide a list of learning rates. \n",
    "\n",
    "---\n",
    "\n",
    "I'm importing the vision section of the fastai library in order to test functions and classes with an existing model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, AnyStr, List, Sequence, TypeVar, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use a simple MNIST dataset to help setup a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_folder(path, size=26, bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `create_cnn` is a fastai function that creates a CNN model we can train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(data, models.resnet18, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this model once using the `fit_one_cycle` method. We don't care about the results, we just want to notice certain things that happen with the `learner` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.basic_train.Learner"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:45\n",
      "epoch  train_loss  valid_loss  error_rate\n",
      "1      6.477190    1.054067    0.034335    (00:45)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=slice(0, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing a learning rate into fit_one_cycle\n",
    "\n",
    "The model of the network is stored in `learn` which is an instance of the class `Learner` which is part of the fastai library. Most functions and methods directly attached to it will be from the fastai library. When we're ready to train our model, we can use `fit_one_cycle()` or `fit()`\n",
    "\n",
    "Let's go deeper in `fit_one_cycle`. It's a function that's written in `train.py` and connected to the `Learner` class through the line:\n",
    "\n",
    "```\n",
    "Learner.fit_one_cycle = fit_one_cycle\n",
    "```\n",
    "\n",
    "Anytime we create an instance of a `Learner` class, this comes attached with it. \n",
    "\n",
    "`Learner.fit_one_cycle` can take many arguments. I'm just going to focus on maximum learning rate `max_lr`, which is a variable that is expected to be either be a float or a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train .fit_one_cycle()\n",
    "def fit_one_cycle(learn:Learner, cyc_len:int, \n",
    "                  max_lr:Union[Floats,slice]=default_lr, \n",
    "                  moms:Tuple[float,float]=(0.95,0.85),\n",
    "                  div_factor:float=25., pct_start:float=0.3, \n",
    "                  wd:float=None,\n",
    "                  callbacks:Optional[CallbackList]=None, **kwargs)-> None:\n",
    "    \"Fit a model following the 1cycle policy.\"\n",
    "    max_lr = learn.lr_range(max_lr)\n",
    "    callbacks = ifnone(callbacks, [])\n",
    "    callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n",
    "                                        pct_start=pct_start, **kwargs))\n",
    "    learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)\n",
    "    \n",
    "# Learner().lr_range() that's used in fit_one_cycle\n",
    "def lr_range(self, lr:Union[float,slice])-> np.ndarray:\n",
    "        \"Build differential learning rates.\"\n",
    "        if not isinstance(lr,slice): return lr\n",
    "        if lr.start: res = even_mults(lr.start, lr.stop, len(self.layer_groups))\n",
    "        else: res = [lr.stop/3]*(len(self.layer_groups)-1) + [lr.stop]\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the maximum learning rate `max_lr` is assigned the value of `default_lr`, or more precisely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(None, 0.003, None)\n"
     ]
    }
   ],
   "source": [
    "print(default_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens to `max_lr` in `fit_one_cycle` depends on the three different scenarios:\n",
    "\n",
    "\n",
    "### If `max_lr` is a `float`\n",
    "\n",
    "If `max_lr` is a float, `learn.lr_range(max_lr)` in the `fit_one_cycle` class method will return float as a float. It won't change it at all.\n",
    "\n",
    "That's because in the line `max_lr = learn.lr_range(max_lr)`, `lr_range` first checks to see if the input `max_lr` is or is not a `slice`. If its not a `slice` object, `lr_range` will immediately return `max_lr` without manipulating the value or datatype.\n",
    "\n",
    "Once it returns the learning rate, the learning rate is passed onto `callbacks` and to `learn.fit`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### If `max_lr` is a `slice` with a non-zero starting index (e.g. slice(1, 2), slice(0.1, 1, 1))\n",
    "\n",
    "If `max_lr` is a slice and contains a starting position that is neither zero nor `None`, e.g. `slice(1, 2)` or `slice(0.01, 1)`, then `lr_range` will pass the starting number, the stop number, and the length of the layer groups into another function called `even_mults`:\n",
    "\n",
    "\n",
    "```\n",
    "if lr.start: res = even_mults(lr.start, lr.stop, len(self.layer_groups))\n",
    "```\n",
    "\n",
    "\n",
    "`even_mults` is a function contained in the fastai core library that will return a numpy array containing a list learning rates between the start number and stop number, with the total number of intervals in between dependent on `n`. When the function is called from `lr_range`, `n` is number of `layer_groups` in the model. It's not clear to me why that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai function contained within Core.py\n",
    "def even_mults(start:float, stop:float, n:int)-> np.ndarray:\n",
    "    \"Build evenly stepped schedule from `start` to `stop` in `n` steps.\"\n",
    "    mult = stop / start\n",
    "    step = mult ** (1 / (n - 1))\n",
    "    return np.array([start * (step ** i) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out with a sample slice that has non-zero start index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01    , 0.017321, 0.03    ])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_slice = slice(0.01, 0.03)\n",
    "even_mults(sample_slice.start, sample_slice.stop, len(learn.layer_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We received three different learning rates: One with our minimum, one somewhere in between (though not precisely), and one that's the maximum. The number of learning rates was ties to the number of layer groups.\n",
    "\n",
    "We'll find that if our network had more than three layer groups, we'd get more than three learning rates. Let's pretend our network has ten layer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01    , 0.011298, 0.012765, 0.014422, 0.016295, 0.018411, 0.020801, 0.023501, 0.026553, 0.03    ])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_slice = slice(0.01, 0.03)\n",
    "even_mults(sample_slice.start, sample_slice.stop, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `even_mults` returned 10 learning rates, 8 of which were between 0.01 and 0.03. My assumption is that this will users to apply different learning rates to different layer groups.\n",
    "\n",
    "This numpy array will be passed back to `fit_one_cycle` to be used in the `OneCycleScheduler` callback, as well as passed to `learn.fit` function.\n",
    "\n",
    "**Sidenote:** Layer groups (e.g. `self.layer_groups`) are an instance attribute of a `Learner` object that contain the Pytorch `Sequential` blocks of the model, which are groupings of layers that operate one after another sequentially. It's not important to know for this exercise, but we can say that the number of groups vary according to the way model is structured in Pytorch.\n",
    "\n",
    "Here's an example Sequential layer in Pytorch:\n",
    "\n",
    "```\n",
    "layer_group = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If `max_lr` is a `slice` without a starting index (e.g. slice(None, 0.5, None) or slice(0, 0.5)) \n",
    "\n",
    "If `max_lr` is a `slice` with a starting index of 0 or `None`, the `lr_range` method will fail the first two conditionals and contiue to the `else` section as follows: \n",
    "\n",
    "```\n",
    "else:\n",
    "    res = [lr.stop/3]*(len(self.layer_groups)-1) + [lr.stop]\n",
    "```\n",
    "\n",
    "Here, we see that we're not including `max_lr.start` in this formula, and instead calculating the list of learning rates purely with `lr.stop` aka the second slice number, and the number of layer groups. In other words, when we don't have a non-zero starting value in the slice we provided `fit_one_cycle`, the `lr_range` function creates a list of values without it.\n",
    "\n",
    "`[lr.stop / 3]` is dividing the max learning rate we provided by 3 and then duplicating that value by the length of networks layer groups minus 1. At the end, it appends the value of the `lr.stop`, the maximum learning rate we provided, to the list. \n",
    "\n",
    "It's not clear why we're dividing by 3 since the number of layer groups changes depending on th model. Perhaps it's just about trying to find some arbitrary number that's less than the maximum learning rate we chose.\n",
    "\n",
    "The following is a step by step calculation of the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divide max learning rate by 3: [0.001]\n",
      "Duplicate the number by the number of layer groups [0.001, 0.001]\n",
      "Append the maximum learning rate to the end of the list: [0.001, 0.001, 0.003]\n"
     ]
    }
   ],
   "source": [
    "sample_stop = default_lr\n",
    "\n",
    "# Step 1\n",
    "learning_rates = [sample_stop.stop / 3]\n",
    "print(\"Divide max learning rate by 3:\", learning_rates)\n",
    "\n",
    "# Step 2\n",
    "learning_rates =  learning_rates * (len(learn.layer_groups) - 1)\n",
    "print(\"Duplicate the number by the number of layer groups\", learning_rates)\n",
    "\n",
    "# Step 3\n",
    "learning_rates =  learning_rates + [sample_stop.stop]\n",
    "print(\"Append the maximum learning rate to the end of the list:\", learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still In Progress\n",
    "\n",
    "---\n",
    "\n",
    "# Once `max_lr` is calculated, how is it used?\n",
    "\n",
    "Once `max_lr` is calculated in `fit_one_cycle`, it's used for the following:\n",
    "\n",
    "1) The learning rate gets passed into the `OneCycleScheduler` callback.\n",
    "\n",
    "2) The learning rate also gets passed into `Learner.fit`, which passes it to another method `create_opt` which creates the optimizer for the `Learner` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner.fit\n",
    "\n",
    "In `Learner.fit`, `lr`, the learning rate input, can either be a float or a slice. Again, like `fit_one_cycle` it has a default learning rate of `default_lr`.\n",
    "\n",
    "We also see that just like `fit_one_cycle`, it pass `lr` to `lr_range`, where if its a float or numpy list, it doesn't change. And if its a `slice`, it does the same procedures as mentioned previously where it converts the slice to a list of numbers.\n",
    "\n",
    "**`self.create_opt(lr, wd)`** is key here because this is where we establish the optimization attribute associated with the model, `self.opt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai Learner.fit()\n",
    "\n",
    "def fit(self, epochs:int, lr:Union[Floats,slice]=default_lr,\n",
    "        wd:Floats=None, callbacks:Collection[Callback]=None)->None:\n",
    "    \"Fit the model on this learner with `lr` learning rate, `wd` weight decay for `epochs` with `callbacks`.\"\n",
    "    lr = self.lr_range(lr)\n",
    "    if wd is None: wd = self.wd\n",
    "    self.create_opt(lr, wd)\n",
    "    callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\n",
    "    fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n",
    "        callbacks=self.callbacks+callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `create_opt`, the list of learning rates is passed to `OptimWrapper` to create an instance property `self.opt` for our `learn` object, which will be passed into the general basic_train.py `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai Learner.create_opt()\n",
    "\n",
    "def create_opt(self, lr:Floats, wd:Floats=0.)->None:\n",
    "    \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n",
    "    self.opt = OptimWrapper.create(self.opt_func, lr, self.layer_groups, \n",
    "                                   wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OptimWrapper` is a \"Basic wrapper around an optimizer to simplify hyper parameter changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "sample = lr_range(None, [0, 2, 4])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.opt.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.count>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "List len mismatch (4 vs 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-f9d12d1d843a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.004\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mcreate_opt\u001b[0;34m(self, lr, wd)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;34m\"Create optimizer with `lr` learning rate and `wd` weight decay.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_wd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_wd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_wd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_wd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSplitFuncOrIdxList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, opt_func, lr, layer_groups, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_groups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.6/site-packages/fastai/core.py\u001b[0m in \u001b[0;36mlistify\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mint\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'List len mismatch ({len(p)} vs {n})'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: List len mismatch (4 vs 3)"
     ]
    }
   ],
   "source": [
    "learn.fit(1, lr=[0.001, 0.002, 0.003, 0.004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai Learner.fit()\n",
    "def fit(self, epochs:int, lr:Union[Floats,slice]=default_lr,\n",
    "        wd:Floats=None, callbacks:Collection[Callback]=None)->None:\n",
    "    \"Fit the model on this learner with `lr` learning rate, `wd` weight decay for `epochs` with `callbacks`.\"\n",
    "    lr = self.lr_range(lr)\n",
    "    if wd is None: wd = self.wd\n",
    "    self.create_opt(lr, wd)\n",
    "    callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\n",
    "    fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n",
    "        callbacks=self.callbacks+callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in `Learner.create_opt`, we can notice a number of things.\n",
    "\n",
    "We create a new instance variable called `self.opt` which is a product of what comes from `OptimWrapper.create`, which is clearly an `OptimWrapper` object.\n",
    "\n",
    "Another thing to notice is that we pass in `self.opt_func` and `self.layer_groups`. `self.opt_func` is the optimization function we want to use for our model, and by default, it's set to AdamW in the fastai code. \n",
    "\n",
    "`self.layer_groups` is a list of the models layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai Learner.create_opt()\n",
    "def create_opt(self, lr:Floats, wd:Floats=0.)->None:\n",
    "    \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n",
    "    self.opt = OptimWrapper.create(self.opt_func, lr, self.layer_groups, \n",
    "                                   wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adam.Adam"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which optimization function are we using?\n",
    "learn.opt_func.func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AdaptiveAvgPool2d(output_size=1)\n",
      "  (1): AdaptiveMaxPool2d(output_size=1)\n",
      "  (2): Lambda()\n",
      "  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): Dropout(p=0.25)\n",
      "  (5): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (6): ReLU(inplace)\n",
      "  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Dropout(p=0.5)\n",
      "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# What's inside a layer_group?\n",
    "print(learn.layer_groups[2])\n",
    "\n",
    "# How many layer groups are there?\n",
    "print(len(learn.layer_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `OptimWrapper.create()`, we see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai OptimWrapper.create()\n",
    "def create(cls, opt_func:Union[type,Callable], lr:Union[float,Tuple,List],\n",
    "           layer_groups:ModuleList, **kwargs:Any)->optim.Optimizer:\n",
    "    \"Create an optim.Optimizer from `opt_func` with `lr`. Set lr on `layer_groups`.\"\n",
    "    split_groups = split_bn_bias(layer_groups)\n",
    "    opt = opt_func([{'params': trainable_params(l), 'lr':0} for l in split_groups])\n",
    "    opt = cls(opt, **kwargs)\n",
    "    opt.lr = listify(lr, layer_groups)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimWrapper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
