{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5\n",
    "\n",
    "* Matrices contain parameters, i.e. Weight Matrices\n",
    "* Activations are calculated\n",
    "* Inputs are a special calculation\n",
    "* Combined can make the universal approximation function\n",
    "\n",
    "## Resnet 34\n",
    "* Weight matrix with 1000 columns categorizing the Imagenet categories\n",
    "* Obviously not useful for when you want to categorize a much smaller number of categories\n",
    "* `create_cnn` deletes the final matrix, creating two weight matrices, one of the them the size of the categories you need\n",
    "* The other layers are good at distinguishing features already\n",
    "* split model into few sections, gives different parts of the model different learning rates, keep the learning rate of earlier layers smaller so that you don't adjust it too much, called **discriminative learning rates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Learning Rate\n",
    "* `max_lr` -> Single number gives all layers the same weight, `slice(1e-3)` gives earle layers /3 while last layer get `1e-31`, `slice(1e-5, 1e-3)` gives a range learning rate to each layer groups\n",
    "* fastai breaks it into three layers groups\n",
    "* One epoch is looking at all batches once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "* Affine function: A linear function (closed under multiplication and addition?)\n",
    "* Mode \n",
    "* One hot encoding: Information pre-processing\n",
    "* Embedding: Look up something in an array, mathematical the same as multiplying a vector by a one-hot-encoded matrix\n",
    "* latent features: hidden things that were there all along, but are revealed through gradient descent\n",
    "* n-one encoded: a way to encode multiple categories\n",
    "* When using sigmoid, you can increase the range to go beyonnd your input parameters to be more accurate. So if ratings are between 0 and 5, you could use `[0, 5.5]`\n",
    "* Learning rates: Either find the deepest chasm or use the steepest slope\n",
    "* Movie bias and include a user bias\n",
    "* How to use the bias: `learn.bias(list_of_items, is_item=True)`, specific to collab filterinng\n",
    "* How to use the weight: `learn.weight(list_of_items, is_item=True)`, specific to collab filterinng\n",
    "* Users are `ids` and results are `items`\n",
    "* PCA: Linear transformation takes in an input matrix and creates a smaller matrix to represent the space, which the axis in say movie recommedation system can represent tastes and genres\n",
    "* `EmbeddingDotBias` - `nn.Module` - not just Python methods, because you don't need `__call__`, you need `__forward__`\n",
    "* 1:03:00 for thorough exploration on embedding\n",
    "* Weight decay: type of regularization, models with more parameters can be overfitted, shouldn't remove parameters when they can provide a lot of non-linearities\n",
    "* Regularization penalizes complexity, sum of square parameters \n",
    "* Take loss function, sum of square parameters multiplied by the weight decay\n",
    "* **Everything in pytorch: Underscore updates in place, e.g.** `tensor.sub_(0.3)` **would update the tensor in place**\n",
    "* `.grad` provides the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "* Difference betwee the predictions and actuals\n",
    "* `loss(x, y) = mse(m(x, w), y) + weight_decay * sum(weight ^ 2)`\n",
    "* Pytorch TensorDataset: Creates a dataset\n",
    "* Sidenote: Must call the super class constructor to set up the nn.Module\n",
    "* Assign the model to cuda: model = MyModel().cuda()\n",
    "* `[p.shape for p in model.parameters()]` to see an abbreviated version of the model parameters\n",
    "* **1:35:00** manually creates an optimization\n",
    "* Cross Entropy Loss: \n",
    "* Pytorch tensors \n",
    "* L2 Regularization vs Weight Decay\n",
    "* Adam Optimizer: Does both Momentum and RMSProp\n",
    "* Momentum (used with momentum): Uses the previous gradient and direction alongside the new calculation\n",
    "* RMSProp: Similar to momentum, but the exponetial moving average of the gradient squared: if gradient is volatile this will be a big number, if it's stable it will be a low number. During the update, \n",
    "* `nn.CrossEntropyLoss()`: 2:04:00 - Predicting the wrong thing with a lot of confidence should have a lot of loss\n",
    "* cxeloss: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_one_cycle\n",
    "* Learning rate starts slow, can increase because gradients are going the way you want to go\n",
    "* Everytime learning rate is slow, momentum is high\n",
    "* 2:02:00 discussion\n",
    "* learn more about exponentially weighted\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
