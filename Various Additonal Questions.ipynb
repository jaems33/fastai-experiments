{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various ML questions\n",
    "\n",
    "### Why use Cross Entropy over say, Mean Squared Error for a Classification problem?\n",
    "\n",
    "https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/\n",
    "\n",
    "Example shows how MSE puts slightly more emphasis on the output instead of the weights that contributed to the classification.\n",
    "\n",
    "MSE is used when node activations can be understood as representing the probability that each hypothesis might be true, i.e. when the output is a probability distribution. Cross-entropy is commonly used to quantify the difference between two probability distributions. \n",
    "\n",
    "The `ln()` function in cross-entropy takes into account the closeness of a prediction and is a more granular way to compute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computed       | targets              | correct?\n",
    "# -----------------------------------------------\n",
    "# 0.3  0.3  0.4  | 0  0  1 (democrat)   | yes\n",
    "# 0.3  0.4  0.3  | 0  1  0 (republican) | yes\n",
    "# 0.1  0.2  0.7  | 1  0  0 (other)      | no\n",
    "\n",
    "# computed       | targets              | correct?\n",
    "# -----------------------------------------------\n",
    "# 0.1  0.2  0.7  | 0  0  1 (democrat)   | yes\n",
    "# 0.1  0.7  0.2  | 0  1  0 (republican) | yes\n",
    "# 0.3  0.4  0.3  | 1  0  0 (other)      | no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use softmax for classification instead of say, standard normalization?\n",
    "\n",
    "![Softmax Formula](https://jamesmccaffrey.files.wordpress.com/2016/03/softmaxequation.jpg?w=279&h=&zoom=2)\n",
    "\n",
    "The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied.\n",
    "\n",
    "An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family\n",
    "https://arxiv.org/pdf/1511.05042.pdf\n",
    "\n",
    "> Our experiments showed that for several low dimensional problems, the log-softmax is surprisingly\n",
    "outperformed by certain losses of the spherical family, in particular the log-Taylor softmax. On the\n",
    "other hand, in higher dimensional problems, the log-softmax yields better results. The reasons of\n",
    "this qualitative shift remain unclear and further research should be carried out to understand it.\n",
    "\n",
    "From StackOverflow:\n",
    "\n",
    "> Reacts to low stimulation (think blurry image) of your neural net with rather uniform distribution and to high stimulation (ie. large numbers, think crisp image) with probabilities close to 0 and 1.\n",
    "\n",
    "> While standard normalisation does not care as long as the proportion are the same.\n",
    "\n",
    "> Have a look what happens when soft max has 10 times larger input, ie your neural net got a crisp image and a lot of neurones got activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference in using softmax from \n",
    "\n",
    "# softmax([1,2])              # blurry image of a ferret\n",
    "# [0.26894142,      0.73105858])  #     it is a cat perhaps !?\n",
    "# softmax([10,20])            # crisp image of a cat\n",
    "# [0.0000453978687, 0.999954602]) #     it is definitely a CAT !\n",
    "\n",
    "# >>> std_norm([1,2])                      # blurry image of a ferret\n",
    "# [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?\n",
    "# >>> std_norm([10,20])                    # crisp image of a cat\n",
    "# [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the Deep Learning Book: \n",
    "The use of cross-entropy losses greatlyimproved the performance of models with sigmoid and softmax outputs, whichhad previously suﬀered from saturation and slow learning when using the meansquared error loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are skip connections?\n",
    "\n",
    "\n",
    "\n",
    "![Skip Connection Diagram](https://i.stack.imgur.com/UDvbg.png)\n",
    "\n",
    "Skip connections are extra connections between nodes in different layers of a neural network that\n",
    "skip one or more layers of nonlinear processing.\n",
    "\n",
    "Their main purpose is on improving gradient flow through the network, which in essence increases the capacity without increasing the number of parameters.\n",
    "\n",
    "Via Resnet's paper: In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. \n",
    "\n",
    "Other answer: Maybe no one knows? One of the papers submitted to ICLR (International Conference on Learning Representations) https://openreview.net/pdf?id=HkwBEMWCZ: We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the “ghosts” of these singularities and sculpt the landscape around them to alleviate the learning slow-down. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
