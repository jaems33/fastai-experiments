{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5\n",
    "\n",
    "* Matrices contain parameters, i.e. Weight Matrices\n",
    "* Activations are calculated\n",
    "* Inputs are a special calculation\n",
    "* Combined can make the universal approximation function\n",
    "\n",
    "## Resnet 34\n",
    "* Weight matrix with 1000 columns categorizing the Imagenet categories\n",
    "* Obviously not useful for when you want to categorize a much smaller number of categories\n",
    "* `create_cnn` deletes the final matrix, creating two weight matrices, one of the them the size of the categories you need\n",
    "* The other layers are good at distinguishing features already\n",
    "* split model into few sections, gives different parts of the model different learning rates, keep the learning rate of earlier layers smaller so that you don't adjust it too much, called **discriminative learning rates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Learning Rate\n",
    "* `max_lr` -> Single number gives all layers the same weight, `slice(1e-3)` gives earle layers /3 while last layer get `1e-31`, `slice(1e-5, 1e-3)` gives a range learning rate to each layer groups\n",
    "* fastai breaks it into three layers groups\n",
    "* One epoch is looking at all batches once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "* Affine function: A linear function (closed under multiplication and addition?)\n",
    "* Mode \n",
    "* One hot encoding: Information pre-processing\n",
    "* Embedding: Look up something in an array, mathematical the same as multiplying a vector by a one-hot-encoded matrix\n",
    "* latent features: hidden things that were there all along, but are revealed through gradient descent\n",
    "* n-one encoded: a way to encode multiple categories\n",
    "* When using sigmoid, you can increase the range to go beyonnd your input parameters to be more accurate. So if ratings are between 0 and 5, you could use `[0, 5.5]`\n",
    "* Learning rates: Either find the deepest chasm or use the steepest slope\n",
    "* Movie bias and include a user bias\n",
    "* How to use the bias: `learn.bias(list_of_items, is_item=True)`, specific to collab filterinng\n",
    "* How to use the weight: `learn.weight(list_of_items, is_item=True)`, specific to collab filterinng\n",
    "* Users are `ids` and results are `items`\n",
    "* PCA: Linear transformation takes in an input matrix and creates a smaller matrix to represent the space, which the axis in say movie recommedation system can represent tastes and genres\n",
    "* `EmbeddingDotBias` - `nn.Module` - not just Python methods, because you don't need `__call__`, you need `__forward__`\n",
    "* 1:03:00 for thorough exploration on embedding\n",
    "* Weight decay: type of regularization, models with more parameters can be overfitted, shouldn't remove parameters when they can provide a lot of non-linearities\n",
    "* Regularization penalizes complexity, sum of square parameters \n",
    "* Take loss function, sum of square parameters multiplied by the weight decay\n",
    "* **Everything in pytorch: Underscore updates in place, e.g.** `tensor.sub_(0.3)` **would update the tensor in place**\n",
    "* `.grad` provides the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "* Difference betwee the predictions and actuals\n",
    "* `loss(x, y) = mse(m(x, w), y) + weight_decay * sum(weight ^ 2)`\n",
    "* Pytorch TensorDataset: Creates a dataset\n",
    "* Sidenote: Must call the super class constructor to set up the nn.Module\n",
    "* Assign the model to cuda: model = MyModel().cuda()\n",
    "* `[p.shape for p in model.parameters()]` to see an abbreviated version of the model parameters\n",
    "* **1:35:00** manually creates an optimization\n",
    "* Cross Entropy Loss: \n",
    "* Pytorch tensors \n",
    "* L2 Regularization vs Weight Decay\n",
    "* Adam Optimizer: Does both Momentum and RMSProp\n",
    "* Momentum (used with momentum): Uses the previous gradient and direction alongside the new calculation\n",
    "* RMSProp: Similar to momentum, but the exponetial moving average of the gradient squared: if gradient is volatile this will be a big number, if it's stable it will be a low number. During the update, \n",
    "* `nn.CrossEntropyLoss()`: 2:04:00 - Predicting the wrong thing with a lot of confidence should have a lot of loss\n",
    "* cxeloss: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_one_cycle\n",
    "* Learning rate starts slow, can increase because gradients are going the way you want to go\n",
    "* Everytime learning rate is slow, momentum is high\n",
    "* 2:02:00 discussion\n",
    "* learn more about exponentially weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "\n",
    "* Rossman data set: Predict how many products to sell in 3 weeks\n",
    "* Data provided was small number of files, can use external data \n",
    "* `rossman_data_clea.ipynb` for data cleaning, \n",
    "* `add_datapart()` \n",
    "* Splittingdates into a different attributes: day, moth, wk, \n",
    "* Category - List of list of catgories. -1 in Pandas means missing\n",
    "* FillMissing will search for a value that missing, whereby you can fill it back in with the median\n",
    "* Must give categorical labels and continuous variables. Some things might not be continuous like Day because \n",
    "* Regularization: Weight decay, Embedding dropout, \n",
    "* Dropout: Throws away a percentge of the activations, no neuron can memorize\n",
    "* ps = dropout value for each layer\n",
    "* Where did the idea come from? Often comes from intuition or real life vs. math\n",
    "* During prediction tim, don't use dropout\n",
    "* BatchNorm does't necessarly reduce Internal Covariate Shift, but helps reduce bumpiness of \n",
    "* Batchnorm's components Gamma and Beta are learned parameters\n",
    "* Batchnorm uses an exponentially weighted moving average\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "* Almost no cost it\n",
    "* Good idea to look at the dataset and then augment it based on overall dataset\n",
    "* in-fill padding: for the blank area around the imagem, you fill the empty space with the edge \n",
    "* in-fill reflection: for the empty space, you fill it with the reflectio of the image\n",
    "\n",
    "### CNN\n",
    "\n",
    "* Lecture 6, 1:03:00ish\n",
    "* Sometimes create stride 2 convolutions that skip over, where the output is half the size\n",
    "* Average Pooling: Convolutional operation with the average of the the filter size.\n",
    "* Hooks: Pytorch feature that hooks into the Pytorch machinery in order to run the code you want. Hook into its output, pass the module you want to. \n",
    "* Hooks: Every single time you run it, it saves it, so you want to make sure you delete it eventually\n",
    "* Transfer learning: \n",
    "* Normally when running a forward pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
