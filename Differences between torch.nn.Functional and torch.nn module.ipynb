{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing between torch.nn.functional vs torch.nn module\n",
    "\n",
    "Credit goes to forum users:\n",
    "\n",
    "Miguel Varela Ramos\n",
    "\n",
    "for their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `__init__` function, you are supposed to initialize the layers you want to use. Unlike keras, Pytorch goes more low level and you have to specify the sizes of your network so that everything matches.\n",
    "\n",
    "In the `forward` method, you specify the connections of your layers. This means that you will use the layers you already initialized, in order to re-use the same layer for each forward pass of data you make.\n",
    "\n",
    "`torch.nn.Functional` contains some useful functions like activation functions a convolution operations you can use. However, `torch.nn.Function` are **not full layers** so if you want to specify a layer of any kind you should use `torch.nn.Module`.\n",
    "\n",
    "You would use the `torch.nn.Functional` conv operations to define a custom layer for example with a convolution operation, but not to define a standard convolution layer.\n",
    "\n",
    "A `nn.Module` is actually a OO wrapper around the functional interface, that contains a number of utility methods, like eval() and parameters(), and it automatically creates the parameters of the modules for you.\n",
    "you can use the functional interface whenever you want, but that requires you to define the weights by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy example where dropout uses the training parameter\n",
    "sample = torch.tensor([1, 2, 3])\n",
    "torch.nn.functional.dropout(sample, p=0.5, training=False, inplace=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Adam Paszke, lead Pytorch Dev:\n",
    "> I think itâ€™s clearer to use modules for stateful function (in this case dropout can be considered stateful, because of this flag), and functional for everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of nn.module\n",
    "\n",
    "`torch.nn.Sequential` A sequential container. Modules will be added to it in the order they are passed in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = nn.Sequential(nn.Conv2d(1, 10, 3),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(10, 64, 3),\n",
    "                            nn.ReLU()\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(10, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways of writing a sample model using Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSequential(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, kernel_size=5, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x)\n",
    "        output = self.layer2(output)\n",
    "        output = output.reshape(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, kernel_size=5, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(1, 16, kernel_size, stride, padding)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.layer2 = nn.Conv2d(16, 32, kernel_size, stride, padding)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.pool1(F.relu(F.batch_norm(self.layer1(x))))\n",
    "        output = self.pool2(F.relu(F.batch_norm(output)))\n",
    "        output = output.reshape(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
